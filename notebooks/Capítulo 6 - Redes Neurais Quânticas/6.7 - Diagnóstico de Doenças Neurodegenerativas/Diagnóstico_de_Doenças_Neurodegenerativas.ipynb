{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njy2t4Axb0kR",
        "outputId": "5a03731a-513a-425a-b0de-eedc3d906368"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.40.0)\n",
            "Requirement already satisfied: numpy<2.1 in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.16.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.2)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.1)\n",
            "Requirement already satisfied: pennylane-lightning>=0.40 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.40.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (24.2)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning>=0.40->pennylane) (0.3.29.0.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (2.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.1.31)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "n_qubits = 5\n",
        "\n",
        "# define o simulador de estados\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "device = 'cpu' # executaremos tudo em cpu\n",
        "\n",
        "def H_layer(nqubits):\n",
        "    \"\"\"Camada de portas Hadamard de um único qubit.\"\"\"\n",
        "    for idx in range(nqubits):\n",
        "        qml.Hadamard(wires=idx)\n",
        "\n",
        "\n",
        "def RY_layer(w):\n",
        "    \"\"\"Camada de rotações parametrizadas de qubit ao redor do eixo y.\"\"\"\n",
        "    for idx, element in enumerate(w):\n",
        "        qml.RY(element, wires=idx)\n",
        "\n",
        "\n",
        "def entangling_layer(nqubits):\n",
        "    \"\"\"Camada de CNOTs seguida por outra camada deslocada de CNOT (gera emaranhamento).\"\"\"\n",
        "    # Em outras palavras, teremos algo como :\n",
        "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
        "    #   CNOT  CNOT  CNOT...  CNOT\n",
        "    for i in range(0, nqubits - 1, 2):  # Iteramos sobre os indices pares: i=0,2,...N-2\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "    for i in range(1, nqubits - 1, 2):  # Iteramos sobre os indices impares:  i=1,3,...N-3\n",
        "        qml.CNOT(wires=[i, i + 1])\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n",
        "    \"\"\"\n",
        "    Nossa Rede Neural Quântica (RNQ).\n",
        "    \"\"\"\n",
        "\n",
        "    # Apenas ajeitamos a dimensão dos pesos para profundidade x n_qubits.\n",
        "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
        "\n",
        "    # Sequência de camadas parametrizadas.\n",
        "    for k in range(q_depth):\n",
        "        # U_0(x) : camada de codificação nas amplitudes do estado.\n",
        "        qml.AmplitudeEmbedding(features=q_input_features, normalize=True, pad_with=0, wires=range(n_qubits))\n",
        "\n",
        "        # camada de emaranhamento seguida de rotações parametrizadas em Y.\n",
        "        entangling_layer(n_qubits)\n",
        "        RY_layer(q_weights[k])\n",
        "\n",
        "    # Retorna o valor esperado em Z\n",
        "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
        "    return tuple(exp_vals)\n",
        "\n",
        "\n",
        "\n",
        "class RNQ(nn.Module):\n",
        "    \"\"\"\n",
        "    Módulo Torch que implementa nossa rede quântica híbrida.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, q_depth, n_qubits, q_delta):\n",
        "        \"\"\"\n",
        "        Inicialização. Perceba que post_net é uma camada clássica ao final do circuito,\n",
        "        projetando a medida em 2 neurônios clássicos, que representarão a probabilidade\n",
        "        do paciente ser patológico ou não.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "        self.q_depth = q_depth\n",
        "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
        "        self.post_net = nn.Linear(n_qubits, 2)\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        \"\"\"\n",
        "        No forward, definimos como os tensores se movem no modelo.\n",
        "        \"\"\"\n",
        "\n",
        "        # Obtém as features de entrada para o circuito quântico\n",
        "        # (observe a normalização necessária)\n",
        "        q_in = torch.tanh(input_features) * np.pi / 2.0\n",
        "\n",
        "        # Aplica o circuito quântico em cada elemento do batch e anexa em q_out\n",
        "        q_out = torch.Tensor(0, n_qubits)\n",
        "        q_out = q_out.to(device)\n",
        "        for elem in q_in:\n",
        "            q_out_elem = torch.tensor(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits))\n",
        "            q_out = torch.cat((q_out, q_out_elem.unsqueeze(0)))\n",
        "\n",
        "        # Retorna a predição da camada clássica de pós-processamento\n",
        "        return self.post_net(q_out.to(torch.float32))"
      ],
      "metadata": {
        "id": "q9hSEu8DcbWe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pvWgdE_vbpIp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv('parkinsons.txt')\n",
        "\n",
        "features=df.loc[:,df.columns!='status'].values[:,1:]\n",
        "labels=df.loc[:,'status'].values\n",
        "\n",
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(features, labels, stratify=labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "reFYZ-WVb48f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "8Cpy74S7b_Hv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "train_loader = data_utils.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "test_ds = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "test_loader = data_utils.DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloaders = {\"train\": train_loader, \"validation\": test_loader}\n",
        "dataset_sizes = {\"train\": len(X_train), \"validation\": len(X_test)}"
      ],
      "metadata": {
        "id": "3UYCRw1Ub_cL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "def train_model(model, dataloaders, dataset_sizes, batch_size, criterion, optimizer, num_epochs):\n",
        "    best_acc = 0.0\n",
        "    best_f1 = 0.0\n",
        "    best_loss = 10000.0  # número grande arbitrário\n",
        "    best_acc_train = 0.0\n",
        "    best_loss_train = 10000.0  # número grande arbitrário\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Cada época tem uma fase de treino\n",
        "        # e outra de validação.\n",
        "        for phase in [\"train\", \"validation\"]:\n",
        "            if phase == \"train\":\n",
        "                # Colocamos o modelo em modo de treinamento.\n",
        "                model.train()\n",
        "            else:\n",
        "                # De modo alternativo, podemos colocá-lo\n",
        "                # em modo de 'evaluation'\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iteramos sobre os dados\n",
        "            n_batches = dataset_sizes[phase] // batch_size\n",
        "            it = 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                batch_size_ = len(inputs)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Utilizaremos os gradientes apenas durante o treino.\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == \"train\":\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * batch_size_\n",
        "                batch_corrects = torch.sum(preds == labels.data).item()\n",
        "                running_corrects += batch_corrects\n",
        "                it += 1\n",
        "\n",
        "            # Mostramos os resultados\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
        "            epoch_f1 = f1_score(labels.cpu(), preds.cpu(), average='weighted')\n",
        "            print(\n",
        "                \"Estágio: {} Época: {}/{} Loss: {:.4f} Acc: {:.4f} F1: {:.4f}       \".format(\n",
        "                    \"train\" if phase == \"train\" else \"validation  \",\n",
        "                    epoch + 1,\n",
        "                    num_epochs,\n",
        "                    epoch_loss,\n",
        "                    epoch_acc,\n",
        "                    epoch_f1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Salva as métricas da melhor época\n",
        "            if phase == \"validation\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "            if phase == \"validation\" and epoch_f1 > best_f1:\n",
        "                best_f1 = epoch_f1\n",
        "\n",
        "\n",
        "            if phase == \"validation\" and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "            if phase == \"train\" and epoch_acc > best_acc_train:\n",
        "                best_acc_train = epoch_acc\n",
        "            if phase == \"train\" and epoch_loss < best_loss_train:\n",
        "                best_loss_train = epoch_loss\n",
        "\n",
        "    return best_acc, best_f1"
      ],
      "metadata": {
        "id": "AJQpqFOTcEZe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = 'cpu'\n",
        "n_qubits = 5\n",
        "q_depth = 3\n",
        "step = 0.001\n",
        "q_delta = 0.01\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "F1 = []\n",
        "Accuracies = []\n",
        "Recall = []\n",
        "Spec = []\n",
        "\n",
        "for q_depth in [4]:\n",
        "\n",
        "  print(\"[*] Treinamento com {} camadas... \\n\".format(q_depth))\n",
        "  rnq = RNQ(q_depth, n_qubits, q_delta)\n",
        "\n",
        "  # podemos utilizar 'cuda', se houver GPU disponível, ou 'cpu'\n",
        "  rnq = rnq.to(device)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  optimizer_hybrid = optim.Adam(rnq.parameters(), lr=step)\n",
        "\n",
        "  acc, f1 = train_model(\n",
        "      rnq, dataloaders, dataset_sizes, batch_size, criterion, optimizer_hybrid, num_epochs=28\n",
        "  )\n",
        "  F1.append(f1)\n",
        "  Accuracies.append(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "309y8uQEcDoP",
        "outputId": "dff4a1d9-ad9d-4528-c838-2bc9923dd9b7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Treinamento com 4 camadas... \n",
            "\n",
            "Estágio: train Época: 1/28 Loss: 0.7143 Acc: 0.4359 F1: 0.5444       \n",
            "Estágio: validation   Época: 1/28 Loss: 0.7197 Acc: 0.3077 F1: 0.1429       \n",
            "Estágio: train Época: 2/28 Loss: 0.7093 Acc: 0.4359 F1: 0.4029       \n",
            "Estágio: validation   Época: 2/28 Loss: 0.7145 Acc: 0.3590 F1: 0.1429       \n",
            "Estágio: train Época: 3/28 Loss: 0.7042 Acc: 0.4744 F1: 0.5742       \n",
            "Estágio: validation   Época: 3/28 Loss: 0.7095 Acc: 0.4872 F1: 0.5195       \n",
            "Estágio: train Época: 4/28 Loss: 0.6993 Acc: 0.4872 F1: 0.5714       \n",
            "Estágio: validation   Época: 4/28 Loss: 0.7047 Acc: 0.5385 F1: 0.4156       \n",
            "Estágio: train Época: 5/28 Loss: 0.6949 Acc: 0.5256 F1: 0.5397       \n",
            "Estágio: validation   Época: 5/28 Loss: 0.6998 Acc: 0.5641 F1: 0.7143       \n",
            "Estágio: train Época: 6/28 Loss: 0.6901 Acc: 0.5769 F1: 0.5000       \n",
            "Estágio: validation   Época: 6/28 Loss: 0.6952 Acc: 0.5641 F1: 0.3429       \n",
            "Estágio: train Época: 7/28 Loss: 0.6855 Acc: 0.5705 F1: 0.5755       \n",
            "Estágio: validation   Época: 7/28 Loss: 0.6907 Acc: 0.5897 F1: 0.5952       \n",
            "Estágio: train Época: 8/28 Loss: 0.6813 Acc: 0.5962 F1: 0.5455       \n",
            "Estágio: validation   Época: 8/28 Loss: 0.6864 Acc: 0.5897 F1: 0.4156       \n",
            "Estágio: train Época: 9/28 Loss: 0.6768 Acc: 0.6026 F1: 0.5455       \n",
            "Estágio: validation   Época: 9/28 Loss: 0.6822 Acc: 0.6154 F1: 0.5195       \n",
            "Estágio: train Época: 10/28 Loss: 0.6731 Acc: 0.6346 F1: 0.5779       \n",
            "Estágio: validation   Época: 10/28 Loss: 0.6780 Acc: 0.6154 F1: 0.7143       \n",
            "Estágio: train Época: 11/28 Loss: 0.6689 Acc: 0.6603 F1: 0.7229       \n",
            "Estágio: validation   Época: 11/28 Loss: 0.6740 Acc: 0.6410 F1: 0.7143       \n",
            "Estágio: train Época: 12/28 Loss: 0.6651 Acc: 0.6667 F1: 0.4236       \n",
            "Estágio: validation   Época: 12/28 Loss: 0.6701 Acc: 0.6410 F1: 0.7143       \n",
            "Estágio: train Época: 13/28 Loss: 0.6611 Acc: 0.6731 F1: 0.6914       \n",
            "Estágio: validation   Época: 13/28 Loss: 0.6663 Acc: 0.6667 F1: 0.5195       \n",
            "Estágio: train Época: 14/28 Loss: 0.6576 Acc: 0.6923 F1: 0.4857       \n",
            "Estágio: validation   Época: 14/28 Loss: 0.6626 Acc: 0.7179 F1: 0.3429       \n",
            "Estágio: train Época: 15/28 Loss: 0.6540 Acc: 0.6987 F1: 0.4416       \n",
            "Estágio: validation   Época: 15/28 Loss: 0.6591 Acc: 0.7179 F1: 0.7143       \n",
            "Estágio: train Época: 16/28 Loss: 0.6505 Acc: 0.6987 F1: 0.5031       \n",
            "Estágio: validation   Época: 16/28 Loss: 0.6556 Acc: 0.7436 F1: 0.5952       \n",
            "Estágio: train Época: 17/28 Loss: 0.6471 Acc: 0.6987 F1: 0.5870       \n",
            "Estágio: validation   Época: 17/28 Loss: 0.6522 Acc: 0.7436 F1: 0.4156       \n",
            "Estágio: train Época: 18/28 Loss: 0.6442 Acc: 0.6987 F1: 0.5031       \n",
            "Estágio: validation   Época: 18/28 Loss: 0.6488 Acc: 0.7436 F1: 0.5952       \n",
            "Estágio: train Época: 19/28 Loss: 0.6407 Acc: 0.6987 F1: 0.6429       \n",
            "Estágio: validation   Época: 19/28 Loss: 0.6457 Acc: 0.7436 F1: 0.2571       \n",
            "Estágio: train Época: 20/28 Loss: 0.6376 Acc: 0.7051 F1: 0.5952       \n",
            "Estágio: validation   Época: 20/28 Loss: 0.6426 Acc: 0.7436 F1: 0.4156       \n",
            "Estágio: train Época: 21/28 Loss: 0.6347 Acc: 0.7115 F1: 0.6064       \n",
            "Estágio: validation   Época: 21/28 Loss: 0.6396 Acc: 0.7436 F1: 0.5952       \n",
            "Estágio: train Época: 22/28 Loss: 0.6315 Acc: 0.7115 F1: 0.6548       \n",
            "Estágio: validation   Época: 22/28 Loss: 0.6367 Acc: 0.7436 F1: 0.7912       \n",
            "Estágio: train Época: 23/28 Loss: 0.6287 Acc: 0.7244 F1: 0.6914       \n",
            "Estágio: validation   Época: 23/28 Loss: 0.6339 Acc: 0.7436 F1: 0.5952       \n",
            "Estágio: train Época: 24/28 Loss: 0.6262 Acc: 0.7244 F1: 0.6250       \n",
            "Estágio: validation   Época: 24/28 Loss: 0.6310 Acc: 0.7436 F1: 0.5952       \n",
            "Estágio: train Época: 25/28 Loss: 0.6233 Acc: 0.7308 F1: 0.8375       \n",
            "Estágio: validation   Época: 25/28 Loss: 0.6282 Acc: 0.7436 F1: 0.4156       \n",
            "Estágio: train Época: 26/28 Loss: 0.6207 Acc: 0.7308 F1: 0.6914       \n",
            "Estágio: validation   Época: 26/28 Loss: 0.6255 Acc: 0.7436 F1: 0.7912       \n",
            "Estágio: train Época: 27/28 Loss: 0.6181 Acc: 0.7372 F1: 0.8242       \n",
            "Estágio: validation   Época: 27/28 Loss: 0.6229 Acc: 0.7436 F1: 0.7912       \n",
            "Estágio: train Época: 28/28 Loss: 0.6158 Acc: 0.7372 F1: 0.6064       \n",
            "Estágio: validation   Época: 28/28 Loss: 0.6203 Acc: 0.7436 F1: 1.0000       \n"
          ]
        }
      ]
    }
  ]
}